{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QT629icZcaSl"
      },
      "source": [
        "Necessary Library Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Fc2KeNtcFLn"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import tensorflow as tf\n",
        "import string\n",
        "import re\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZX0slNVcbgW"
      },
      "source": [
        "Mount the Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-P7AydiWcbxY",
        "outputId": "06f3c8e2-41fc-428a-a17e-af0942390397"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ik52dtbcb7q"
      },
      "source": [
        "Read the data file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qWYVkWpccEo",
        "outputId": "7041cfb4-8471-4d45-a7d7-bac1f1333abb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 1: I play the bass.\tමම බාස් වාදනය කරනවා.\n",
            "Row 2: Are you embarrassed?\tඔබ ලැජ්ජාවට පත් වෙනවාද?\n",
            "Row 3: I'm invited to a party tonight.\tමම අද රෑ සාදයකට ආරාධනා කරෙයි.\n",
            "Row 4: It isn't easy to write a love letter in English.\tආදර ලිපියක් ඉංග්රීසියෙන් ලිවීම පහසු නැත.\n",
            "Row 5: The dog kept barking all through the night.\tබල්ලා රාත්රිය පුරාම බාධාවක් විය.\n",
            "Row 6: Tom disappointed Mary.\tටොම් මරියා කලකිරීමට පත් විය.\n",
            "Row 7: Don't smoke in this room.\tමේ කාමරයේ දුම් බොන්න එපා.\n",
            "Row 8: I gave my father a silk tie.\tමම මගේ පියාට සේද ටයි පටියක් දුන්නා.\n",
            "Row 9: Tom is different from other boys.\tටොම් අනෙක් පිරිමි ළමයින්ට වඩා වෙනස් ය.\n",
            "Row 10: That plane is enormous!\tඑම යානය අතිමහත් ය!\n",
            "Row 11: I want you to get in the car.\tමට ඕන ඔයා කාර් එකට නගින්න.\n",
            "Row 12: He was expelled from school.\tඔහු පාසලෙන් නෙරපා හරින ලදී.\n",
            "Row 13: He doesn't like to lose.\tඔහු පරාජය වීමට කැමති නැත.\n",
            "Row 14: I didn't go to school yesterday.\tමම ඊයේ පාසලට ගියේ නැහැ.\n",
            "Row 15: Doesn't that make you happy?\tඑය ඔබව සතුටු කරන්නේ නැද්ද?\n",
            "Row 16: Why do you love me?\tඇයි ඔබ මට ආදරය කරන්නේ?\n",
            "Row 17: If you go fishing tomorrow, I'll go, too.\tඔබ හෙට මසුන් ඇල්ලන්නේ නම්, මමත් යන්නම්.\n",
            "Row 18: Tom is a good painter.\tටොම් යනු හොඳ චිත්ර ශිල්පියෙකි.\n",
            "Row 19: Tom pleaded not guilty.\tටොම් වරද පිළිගන්නේ නැත.\n",
            "Row 20: Tom's dog attacked Mary.\tටොම්ගේ බල්ලා මරියාට පහර දුන්නා.\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import random\n",
        "\n",
        "csv_file = \"/content/drive/MyDrive/MiniProject03/data.csv\"\n",
        "\n",
        "# Read the CSV file into a list of rows\n",
        "with open(csv_file, 'r') as file:\n",
        "    csv_reader = list(csv.reader(file))\n",
        "\n",
        "# Shuffle the rows randomly\n",
        "random.shuffle(csv_reader)\n",
        "\n",
        "# Process the shuffled rows, limiting to 20 lines\n",
        "for i, row in enumerate(csv_reader[:20], start=1):\n",
        "    cell1, cell2 = row\n",
        "    print(f\"Row {i}: {cell1}\\t{cell2}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUtteFotccdz"
      },
      "source": [
        "Split the English and Sinhala translation pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3PhpQHtccmD",
        "outputId": "46b9274b-af61-4ca1-d91c-a1d3e9609b10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('You might want to talk to Tom.', '[start] ඔබට ටොම් සමඟ කතා කිරීමට අවශ්ය විය හැකිය. [end]')\n",
            "('She makes her mother happy.', '[start] ඇය මව සතුටට පත් කරයි. [end]')\n",
            "('My father is sweeping the garage.', '[start] මගේ තාත්තා ගරාජය අතුගා දමයි. [end]')\n",
            "('There are many parks in London.', '[start] ලන්ඩනයේ බොහෝ උද්යාන තිබේ. [end]')\n",
            "('They took Tom away on a stretcher.', '[start] ඔවුන් ස්ට්රෙචරයක් මත ටොම්ව රැගෙන ගියා. [end]')\n"
          ]
        }
      ],
      "source": [
        "csv_file = \"/content/drive/MyDrive/MiniProject03/data.csv\"\n",
        "\n",
        "text_pairs = []\n",
        "with open(csv_file, 'r') as file:\n",
        "    csv_reader = csv.reader(file)\n",
        "    for row in csv_reader:\n",
        "        if len(row) >= 2 and row[1] is not None:\n",
        "            english, sinhala = row[:2]\n",
        "            sinhala = \"[start] \" + sinhala + \" [end]\"\n",
        "            text_pairs.append((english, sinhala))\n",
        "\n",
        "for _ in range(5):\n",
        "    print(random.choice(text_pairs))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXZhHwAvccuS"
      },
      "source": [
        "Randomize the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntlhKik_cc26"
      },
      "outputs": [],
      "source": [
        "random.shuffle(text_pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTCO23TLcc93"
      },
      "source": [
        "Spliting the data into training, validation and Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-n5GsD7cdFn",
        "outputId": "4459908c-d41b-4570-8a25-308175fc57b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sentences: 102903\n",
            "Training set size: 72033\n",
            "Validation set size: 15435\n",
            "Testing set size: 15435\n"
          ]
        }
      ],
      "source": [
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples:]\n",
        "\n",
        "print(\"Total sentences:\",len(text_pairs))\n",
        "print(\"Training set size:\",len(train_pairs))\n",
        "print(\"Validation set size:\",len(val_pairs))\n",
        "print(\"Testing set size:\",len(test_pairs))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nlz7_hQBcdTu",
        "outputId": "8cdc6821-16ec-4259-e119-76b7746cf870"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "102903"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "len(train_pairs)+len(val_pairs)+len(test_pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfKFGFWTkifQ"
      },
      "source": [
        "Removing Punctuations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "IZrUBdcDcdiB",
        "outputId": "b63e7053-1c52-404a-f968-b08c3a83ebf1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[!\"\\\\#\\\\$%\\\\&\\'\\\\(\\\\)\\\\*\\\\+,\\\\-\\\\./:;<=>\\\\?@\\\\\\\\\\\\^_`\\\\{\\\\|\\\\}\\\\~¿]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "f\"[{re.escape(strip_chars)}]\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9N4er4ccd9E"
      },
      "source": [
        "Vectorizing the English and Sinhala text pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2VJVtguceFs"
      },
      "outputs": [],
      "source": [
        "def custom_standardization(input_string):\n",
        "  lowercase = tf.strings.lower(input_string)\n",
        "  return tf.strings.regex_replace(\n",
        "      lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
        "\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "\n",
        "source_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "target_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "train_english_texts = [pair[0] for pair in train_pairs]\n",
        "train_sinhala_texts = [pair[1] for pair in train_pairs]\n",
        "source_vectorization.adapt(train_english_texts)\n",
        "target_vectorization.adapt(train_sinhala_texts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xu8bHjxmceNT"
      },
      "source": [
        "Preparing datasets for the translation task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsStSjhAceVI",
        "outputId": "bd626786-dff8-438d-dcb3-c9cee42e14ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs['english'].shape: (64, 20)\n",
            "inputs['sinhala'].shape: (64, 20)\n",
            "targets.shape: (64, 20)\n"
          ]
        }
      ],
      "source": [
        "batch_size = 64\n",
        "\n",
        "def format_dataset(eng, sin):\n",
        "   eng = source_vectorization(eng)\n",
        "   sin = target_vectorization(sin)\n",
        "   return ({\n",
        "         \"english\": eng,\n",
        "         \"sinhala\": sin[:, :-1],\n",
        "   }, sin[:, 1:])\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, sin_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    sin_texts = list(sin_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, sin_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)\n",
        "\n",
        "\n",
        "for inputs, targets in train_ds.take(1):\n",
        "   print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
        "   print(f\"inputs['sinhala'].shape: {inputs['sinhala'].shape}\")\n",
        "   print(f\"targets.shape: {targets.shape}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-M6gEmFcekl",
        "outputId": "25453e64-5b19-45c5-f500-f0a34df2085f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "({'english': array([[   9,   70,  305, ...,    0,    0,    0],\n",
            "       [ 313,  101,    0, ...,    0,    0,    0],\n",
            "       [   5,   32,   24, ...,    0,    0,    0],\n",
            "       ...,\n",
            "       [  27, 3057,  583, ...,    0,    0,    0],\n",
            "       [   3,   99,  987, ...,    0,    0,    0],\n",
            "       [4349, 2082,   49, ...,    0,    0,    0]]), 'sinhala': array([[   2,    8,  861, ...,    0,    0,    0],\n",
            "       [   2,  135,  865, ...,    0,    0,    0],\n",
            "       [   2,    5,   58, ...,    0,    0,    0],\n",
            "       ...,\n",
            "       [   2,   16, 8494, ...,    0,    0,    0],\n",
            "       [   2,    4,   10, ...,    0,    0,    0],\n",
            "       [   2,  889,  218, ...,    0,    0,    0]])}, array([[   8,  861,  441, ...,    0,    0,    0],\n",
            "       [ 135,  865,    3, ...,    0,    0,    0],\n",
            "       [   5,   58,  101, ...,    0,    0,    0],\n",
            "       ...,\n",
            "       [  16, 8494,  648, ...,    0,    0,    0],\n",
            "       [   4,   10, 4955, ...,    0,    0,    0],\n",
            "       [ 889,  218, 1957, ...,    0,    0,    0]]))\n"
          ]
        }
      ],
      "source": [
        "print(list(train_ds.as_numpy_iterator())[50])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPb7zyyocesc"
      },
      "source": [
        "Transformer encoder implemented as a subclassed Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xj6kbzgIcez0"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "      super().__init__(**kwargs)\n",
        "      self.embed_dim = embed_dim\n",
        "      self.dense_dim = dense_dim\n",
        "      self.num_heads = num_heads\n",
        "      self.attention = layers.MultiHeadAttention(\n",
        "           num_heads=num_heads, key_dim=embed_dim)\n",
        "      self.dense_proj = keras.Sequential(\n",
        "           [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "            layers.Dense(embed_dim),]\n",
        "      )\n",
        "      self.layernorm_1 = layers.LayerNormalization()\n",
        "      self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "      if mask is not None:\n",
        "         mask = mask[:, tf.newaxis, :]\n",
        "      attention_output = self.attention(\n",
        "         inputs, inputs, attention_mask=mask)\n",
        "      proj_input = self.layernorm_1(inputs + attention_output)\n",
        "      proj_output = self.dense_proj(proj_input)\n",
        "      return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fub6n634ce7j"
      },
      "source": [
        "The Transformer decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjCAl5PycfDe"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "      super().__init__(**kwargs)\n",
        "      self.embed_dim = embed_dim\n",
        "      self.dense_dim = dense_dim\n",
        "      self.num_heads = num_heads\n",
        "      self.attention_1 = layers.MultiHeadAttention(\n",
        "          num_heads=num_heads, key_dim=embed_dim)\n",
        "      self.attention_2 = layers.MultiHeadAttention(\n",
        "          num_heads=num_heads, key_dim=embed_dim)\n",
        "      self.dense_proj = keras.Sequential(\n",
        "          [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "           layers.Dense(embed_dim),]\n",
        "      )\n",
        "      self.layernorm_1 = layers.LayerNormalization()\n",
        "      self.layernorm_2 = layers.LayerNormalization()\n",
        "      self.layernorm_3 = layers.LayerNormalization()\n",
        "      self.supports_masking = True\n",
        "\n",
        "    def get_config(self):\n",
        "      config = super().get_config()\n",
        "      config.update({\n",
        "          \"embed_dim\": self.embed_dim,\n",
        "          \"num_heads\": self.num_heads,\n",
        "          \"dense_dim\": self.dense_dim,\n",
        "      })\n",
        "      return config\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "      input_shape = tf.shape(inputs)\n",
        "      batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "      i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "      j = tf.range(sequence_length)\n",
        "      mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "      mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "      mult = tf.concat(\n",
        "              [tf.expand_dims(batch_size, -1),\n",
        "               tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "      return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "      causal_mask = self.get_causal_attention_mask(inputs)\n",
        "      if mask is not None:\n",
        "           padding_mask = tf.cast(\n",
        "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "           padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "      else:\n",
        "           padding_mask = mask\n",
        "      attention_output_1 = self.attention_1(\n",
        "           query=inputs,\n",
        "           value=inputs,\n",
        "           key=inputs,\n",
        "           attention_mask=causal_mask)\n",
        "      attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "      attention_output_2 = self.attention_2(\n",
        "            query=attention_output_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "       )\n",
        "      attention_output_2 = self.layernorm_2(\n",
        "          attention_output_1 + attention_output_2)\n",
        "      proj_output = self.dense_proj(attention_output_2)\n",
        "      return self.layernorm_3(attention_output_2 + proj_output)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEXwSKpfcfKw"
      },
      "source": [
        "Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlHKt-ticfTu"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "     def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "         super().__init__(**kwargs)\n",
        "         self.token_embeddings = layers.Embedding(\n",
        "             input_dim=input_dim, output_dim=output_dim)\n",
        "         self.position_embeddings = layers.Embedding(\n",
        "             input_dim=sequence_length, output_dim=output_dim)\n",
        "         self.sequence_length = sequence_length\n",
        "         self.input_dim = input_dim\n",
        "         self.output_dim = output_dim\n",
        "\n",
        "     def call(self, inputs):\n",
        "         length = tf.shape(inputs)[-1]\n",
        "         positions = tf.range(start=0, limit=length, delta=1)\n",
        "         embedded_tokens = self.token_embeddings(inputs)\n",
        "         embedded_positions = self.position_embeddings(positions)\n",
        "         return embedded_tokens + embedded_positions\n",
        "\n",
        "     def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "     def get_config(self):\n",
        "        config = super(PositionalEmbedding, self).get_config()\n",
        "        config.update({\n",
        "             \"output_dim\": self.output_dim,\n",
        "             \"sequence_length\": self.sequence_length,\n",
        "             \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaRS3BJRcfbW"
      },
      "source": [
        "End-to-end Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBydQxTPcfkC"
      },
      "outputs": [],
      "source": [
        "embed_dim = 256\n",
        "dense_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"sinhala\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbvhUH8fcf0E",
        "outputId": "077deb10-ea43-49ea-f332-7005d1a7c960"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;2;248;248;242mModel\u001b[39m\u001b[38;2;248;248;242m:\u001b[39m\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;230;219;116m\"\u001b[39m\u001b[38;2;230;219;116mmodel\u001b[39m\u001b[38;2;230;219;116m\"\u001b[39m\n",
            "\u001b[38;2;248;248;242m__________________________________________________________________________________________________\u001b[39m\n",
            "\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;248;248;242mLayer\u001b[39m\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;248;248;242m(\u001b[39m\u001b[38;2;248;248;242mtype\u001b[39m\u001b[38;2;248;248;242m)\u001b[39m\u001b[38;2;248;248;242m                \u001b[39m\u001b[38;2;248;248;242mOutput\u001b[39m\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;248;248;242mShape\u001b[39m\u001b[38;2;248;248;242m                 \u001b[39m\u001b[38;2;248;248;242mParam\u001b[39m\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;149;144;119m#   Connected to                  \u001b[39m\n",
            "\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\n",
            "\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;248;248;242menglish\u001b[39m\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;248;248;242m(\u001b[39m\u001b[38;2;248;248;242mInputLayer\u001b[39m\u001b[38;2;248;248;242m)\u001b[39m\u001b[38;2;248;248;242m        \u001b[39m\u001b[38;2;248;248;242m[\u001b[39m\u001b[38;2;248;248;242m(\u001b[39m\u001b[38;2;102;217;239mNone\u001b[39m\u001b[38;2;248;248;242m,\u001b[39m\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;102;217;239mNone\u001b[39m\u001b[38;2;248;248;242m)\u001b[39m\u001b[38;2;248;248;242m]\u001b[39m\u001b[38;2;248;248;242m               \u001b[39m\u001b[38;2;174;129;255m0\u001b[39m\u001b[38;2;248;248;242m         \u001b[39m\u001b[38;2;248;248;242m[\u001b[39m\u001b[38;2;248;248;242m]\u001b[39m\u001b[38;2;248;248;242m                            \u001b[39m\n",
            "\u001b[38;2;248;248;242m                                                                                                  \u001b[39m\n",
            "\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;248;248;242msinhala\u001b[39m\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;248;248;242m(\u001b[39m\u001b[38;2;248;248;242mInputLayer\u001b[39m\u001b[38;2;248;248;242m)\u001b[39m\u001b[38;2;248;248;242m        \u001b[39m\u001b[38;2;248;248;242m[\u001b[39m\u001b[38;2;248;248;242m(\u001b[39m\u001b[38;2;102;217;239mNone\u001b[39m\u001b[38;2;248;248;242m,\u001b[39m\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;102;217;239mNone\u001b[39m\u001b[38;2;248;248;242m)\u001b[39m\u001b[38;2;248;248;242m]\u001b[39m\u001b[38;2;248;248;242m               \u001b[39m\u001b[38;2;174;129;255m0\u001b[39m\u001b[38;2;248;248;242m         \u001b[39m\u001b[38;2;248;248;242m[\u001b[39m\u001b[38;2;248;248;242m]\u001b[39m\u001b[38;2;248;248;242m                            \u001b[39m\n",
            "\u001b[38;2;248;248;242m                                                                                                  \u001b[39m\n",
            "\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;248;248;242mpositional_embedding\u001b[39m\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;248;248;242m(\u001b[39m\u001b[38;2;248;248;242mPosi\u001b[39m\u001b[38;2;248;248;242m  \u001b[39m\u001b[38;2;248;248;242m(\u001b[39m\u001b[38;2;102;217;239mNone\u001b[39m\u001b[38;2;248;248;242m,\u001b[39m\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;102;217;239mNone\u001b[39m\u001b[38;2;248;248;242m,\u001b[39m\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;174;129;255m256\u001b[39m\u001b[38;2;248;248;242m)\u001b[39m\u001b[38;2;248;248;242m            \u001b[39m\u001b[38;2;174;129;255m3845120\u001b[39m\u001b[38;2;248;248;242m   \u001b[39m\u001b[38;2;248;248;242m[\u001b[39m\u001b[38;2;230;219;116m'\u001b[39m\u001b[38;2;230;219;116menglish[0][0]\u001b[39m\u001b[38;2;230;219;116m'\u001b[39m\u001b[38;2;248;248;242m]\u001b[39m\u001b[38;2;248;248;242m             \u001b[39m\n",
            "\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;248;248;242mtionalEmbedding\u001b[39m\u001b[38;2;248;248;242m)\u001b[39m\u001b[38;2;248;248;242m                                                                                 \u001b[39m\n",
            "\u001b[38;2;248;248;242m                                                                                                  \u001b[39m\n",
            "\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;248;248;242mpositional_embedding_1\u001b[39m\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;248;248;242m(\u001b[39m\u001b[38;2;248;248;242mPo\u001b[39m\u001b[38;2;248;248;242m  \u001b[39m\u001b[38;2;248;248;242m(\u001b[39m\u001b[38;2;102;217;239mNone\u001b[39m\u001b[38;2;248;248;242m,\u001b[39m\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;102;217;239mNone\u001b[39m\u001b[38;2;248;248;242m,\u001b[39m\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;174;129;255m256\u001b[39m\u001b[38;2;248;248;242m)\u001b[39m\u001b[38;2;248;248;242m            \u001b[39m\u001b[38;2;174;129;255m3845120\u001b[39m\u001b[38;2;248;248;242m   \u001b[39m\u001b[38;2;248;248;242m[\u001b[39m\u001b[38;2;230;219;116m'\u001b[39m\u001b[38;2;230;219;116msinhala[0][0]\u001b[39m\u001b[38;2;230;219;116m'\u001b[39m\u001b[38;2;248;248;242m]\u001b[39m\u001b[38;2;248;248;242m             \u001b[39m\n",
            "\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;248;248;242msitionalEmbedding\u001b[39m\u001b[38;2;248;248;242m)\u001b[39m\u001b[38;2;248;248;242m                                                                               \u001b[39m\n",
            "\u001b[38;2;248;248;242m                                                                                                  \u001b[39m\n",
            "\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;248;248;242mtransformer_encoder\u001b[39m\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;248;248;242m(\u001b[39m\u001b[38;2;248;248;242mTrans\u001b[39m\u001b[38;2;248;248;242m  \u001b[39m\u001b[38;2;248;248;242m(\u001b[39m\u001b[38;2;102;217;239mNone\u001b[39m\u001b[38;2;248;248;242m,\u001b[39m\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;102;217;239mNone\u001b[39m\u001b[38;2;248;248;242m,\u001b[39m\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;174;129;255m256\u001b[39m\u001b[38;2;248;248;242m)\u001b[39m\u001b[38;2;248;248;242m            \u001b[39m\u001b[38;2;174;129;255m3155456\u001b[39m\u001b[38;2;248;248;242m   \u001b[39m\u001b[38;2;248;248;242m[\u001b[39m\u001b[38;2;230;219;116m'\u001b[39m\u001b[38;2;230;219;116mpositional_embedding[0][0]\u001b[39m\u001b[38;2;230;219;116m'\u001b[39m\u001b[38;2;248;248;242m]\u001b[39m\n",
            "\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;248;248;242mformerEncoder\u001b[39m\u001b[38;2;248;248;242m)\u001b[39m\u001b[38;2;248;248;242m                                                                                   \u001b[39m\n",
            "\u001b[38;2;248;248;242m                                                                                                  \u001b[39m\n",
            "\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;248;248;242mtransformer_decoder\u001b[39m\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;248;248;242m(\u001b[39m\u001b[38;2;248;248;242mTrans\u001b[39m\u001b[38;2;248;248;242m  \u001b[39m\u001b[38;2;248;248;242m(\u001b[39m\u001b[38;2;102;217;239mNone\u001b[39m\u001b[38;2;248;248;242m,\u001b[39m\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;102;217;239mNone\u001b[39m\u001b[38;2;248;248;242m,\u001b[39m\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;174;129;255m256\u001b[39m\u001b[38;2;248;248;242m)\u001b[39m\u001b[38;2;248;248;242m            \u001b[39m\u001b[38;2;174;129;255m5259520\u001b[39m\u001b[38;2;248;248;242m   \u001b[39m\u001b[38;2;248;248;242m[\u001b[39m\u001b[38;2;230;219;116m'\u001b[39m\u001b[38;2;230;219;116mpositional_embedding_1[0][0]\u001b[39m\n",
            "\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;248;248;242mformerDecoder\u001b[39m\u001b[38;2;248;248;242m)\u001b[39m\u001b[38;2;248;248;242m                                                     \u001b[39m\u001b[38;2;230;219;116m'\u001b[39m\u001b[38;2;230;219;116m,                            \u001b[39m\n",
            "\u001b[38;2;248;248;242m                                                                     \u001b[39m\u001b[38;2;230;219;116m'\u001b[39m\u001b[38;2;230;219;116mtransformer_encoder[0][0]\u001b[39m\u001b[38;2;230;219;116m'\u001b[39m\u001b[38;2;248;248;242m]\u001b[39m\u001b[38;2;248;248;242m \u001b[39m\n",
            "\u001b[38;2;248;248;242m                                                                                                  \u001b[39m\n",
            "\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;248;248;242mdropout\u001b[39m\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;248;248;242m(\u001b[39m\u001b[38;2;248;248;242mDropout\u001b[39m\u001b[38;2;248;248;242m)\u001b[39m\u001b[38;2;248;248;242m           \u001b[39m\u001b[38;2;248;248;242m(\u001b[39m\u001b[38;2;102;217;239mNone\u001b[39m\u001b[38;2;248;248;242m,\u001b[39m\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;102;217;239mNone\u001b[39m\u001b[38;2;248;248;242m,\u001b[39m\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;174;129;255m256\u001b[39m\u001b[38;2;248;248;242m)\u001b[39m\u001b[38;2;248;248;242m            \u001b[39m\u001b[38;2;174;129;255m0\u001b[39m\u001b[38;2;248;248;242m         \u001b[39m\u001b[38;2;248;248;242m[\u001b[39m\u001b[38;2;230;219;116m'\u001b[39m\u001b[38;2;230;219;116mtransformer_decoder[0][0]\u001b[39m\u001b[38;2;230;219;116m'\u001b[39m\u001b[38;2;248;248;242m]\u001b[39m\u001b[38;2;248;248;242m \u001b[39m\n",
            "\u001b[38;2;248;248;242m                                                                                                  \u001b[39m\n",
            "\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;248;248;242mdense_4\u001b[39m\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;248;248;242m(\u001b[39m\u001b[38;2;248;248;242mDense\u001b[39m\u001b[38;2;248;248;242m)\u001b[39m\u001b[38;2;248;248;242m             \u001b[39m\u001b[38;2;248;248;242m(\u001b[39m\u001b[38;2;102;217;239mNone\u001b[39m\u001b[38;2;248;248;242m,\u001b[39m\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;102;217;239mNone\u001b[39m\u001b[38;2;248;248;242m,\u001b[39m\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;174;129;255m15000\u001b[39m\u001b[38;2;248;248;242m)\u001b[39m\u001b[38;2;248;248;242m          \u001b[39m\u001b[38;2;174;129;255m3855000\u001b[39m\u001b[38;2;248;248;242m   \u001b[39m\u001b[38;2;248;248;242m[\u001b[39m\u001b[38;2;230;219;116m'\u001b[39m\u001b[38;2;230;219;116mdropout[0][0]\u001b[39m\u001b[38;2;230;219;116m'\u001b[39m\u001b[38;2;248;248;242m]\u001b[39m\u001b[38;2;248;248;242m             \u001b[39m\n",
            "\u001b[38;2;248;248;242m                                                                                                  \u001b[39m\n",
            "\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\u001b[38;2;255;70;137m==\u001b[39m\n",
            "\u001b[38;2;248;248;242mTotal\u001b[39m\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;248;248;242mparams\u001b[39m\u001b[38;2;248;248;242m:\u001b[39m\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;174;129;255m19960216\u001b[39m\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;248;248;242m(\u001b[39m\u001b[38;2;174;129;255m76.14\u001b[39m\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;248;248;242mMB\u001b[39m\u001b[38;2;248;248;242m)\u001b[39m\n",
            "\u001b[38;2;248;248;242mTrainable\u001b[39m\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;248;248;242mparams\u001b[39m\u001b[38;2;248;248;242m:\u001b[39m\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;174;129;255m19960216\u001b[39m\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;248;248;242m(\u001b[39m\u001b[38;2;174;129;255m76.14\u001b[39m\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;248;248;242mMB\u001b[39m\u001b[38;2;248;248;242m)\u001b[39m\n",
            "\u001b[38;2;248;248;242mNon\u001b[39m\u001b[38;2;255;70;137m-\u001b[39m\u001b[38;2;248;248;242mtrainable\u001b[39m\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;248;248;242mparams\u001b[39m\u001b[38;2;248;248;242m:\u001b[39m\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;174;129;255m0\u001b[39m\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;248;248;242m(\u001b[39m\u001b[38;2;174;129;255m0.00\u001b[39m\u001b[38;2;248;248;242m \u001b[39m\u001b[38;2;248;248;242mByte\u001b[39m\u001b[38;2;248;248;242m)\u001b[39m\n",
            "\u001b[38;2;248;248;242m__________________________________________________________________________________________________\u001b[39m\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "from pygments import highlight\n",
        "from pygments.lexers import PythonLexer\n",
        "from pygments.formatters import TerminalTrueColorFormatter\n",
        "\n",
        "# Get the summary text\n",
        "summary_text = []\n",
        "transformer.summary(print_fn=lambda x: summary_text.append(x))\n",
        "\n",
        "# Highlight the summary text using Pygments\n",
        "highlighted_summary = highlight('\\n'.join(summary_text), PythonLexer(), TerminalTrueColorFormatter(style='monokai'))\n",
        "\n",
        "# Print the highlighted summary\n",
        "print(highlighted_summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izcfAoCCcf7a"
      },
      "source": [
        "Training the sequence-to-sequence Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtD5TtarcgFw",
        "outputId": "eb42a715-8540-4ec4-f38f-c24bb57042fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "1126/1126 [==============================] - 113s 90ms/step - loss: 2.2349 - accuracy: 0.6233 - val_loss: 2.1400 - val_accuracy: 0.6307\n",
            "Epoch 2/30\n",
            "1126/1126 [==============================] - 81s 72ms/step - loss: 1.8248 - accuracy: 0.6619 - val_loss: 1.9965 - val_accuracy: 0.6442\n",
            "Epoch 3/30\n",
            "1126/1126 [==============================] - 82s 73ms/step - loss: 1.5342 - accuracy: 0.6917 - val_loss: 1.9253 - val_accuracy: 0.6540\n",
            "Epoch 4/30\n",
            "1126/1126 [==============================] - 84s 75ms/step - loss: 1.3261 - accuracy: 0.7166 - val_loss: 1.9221 - val_accuracy: 0.6596\n",
            "Epoch 5/30\n",
            "1126/1126 [==============================] - 82s 73ms/step - loss: 1.1637 - accuracy: 0.7391 - val_loss: 1.9260 - val_accuracy: 0.6602\n",
            "Epoch 6/30\n",
            "1126/1126 [==============================] - 84s 75ms/step - loss: 1.0386 - accuracy: 0.7579 - val_loss: 1.9236 - val_accuracy: 0.6662\n",
            "Epoch 7/30\n",
            "1126/1126 [==============================] - 82s 73ms/step - loss: 0.9384 - accuracy: 0.7761 - val_loss: 1.9805 - val_accuracy: 0.6676\n",
            "Epoch 8/30\n",
            "1126/1126 [==============================] - 82s 73ms/step - loss: 0.8617 - accuracy: 0.7899 - val_loss: 1.9874 - val_accuracy: 0.6708\n",
            "Epoch 9/30\n",
            "1126/1126 [==============================] - 82s 73ms/step - loss: 0.7953 - accuracy: 0.8027 - val_loss: 2.0008 - val_accuracy: 0.6725\n",
            "Epoch 10/30\n",
            "1126/1126 [==============================] - 80s 71ms/step - loss: 0.7368 - accuracy: 0.8148 - val_loss: 2.0154 - val_accuracy: 0.6725\n",
            "Epoch 11/30\n",
            "1126/1126 [==============================] - 84s 74ms/step - loss: 0.6947 - accuracy: 0.8230 - val_loss: 2.0476 - val_accuracy: 0.6743\n",
            "Epoch 12/30\n",
            "1126/1126 [==============================] - 82s 73ms/step - loss: 0.6533 - accuracy: 0.8315 - val_loss: 2.0913 - val_accuracy: 0.6733\n",
            "Epoch 13/30\n",
            "1126/1126 [==============================] - 82s 73ms/step - loss: 0.6173 - accuracy: 0.8394 - val_loss: 2.1072 - val_accuracy: 0.6725\n",
            "Epoch 14/30\n",
            "1126/1126 [==============================] - 82s 73ms/step - loss: 0.5817 - accuracy: 0.8474 - val_loss: 2.1426 - val_accuracy: 0.6746\n",
            "Epoch 15/30\n",
            "1126/1126 [==============================] - 82s 73ms/step - loss: 0.5541 - accuracy: 0.8534 - val_loss: 2.1955 - val_accuracy: 0.6742\n",
            "Epoch 16/30\n",
            "1126/1126 [==============================] - 82s 73ms/step - loss: 0.5254 - accuracy: 0.8598 - val_loss: 2.1997 - val_accuracy: 0.6739\n",
            "Epoch 17/30\n",
            "1126/1126 [==============================] - 82s 73ms/step - loss: 0.5021 - accuracy: 0.8652 - val_loss: 2.2086 - val_accuracy: 0.6746\n",
            "Epoch 18/30\n",
            "1126/1126 [==============================] - 80s 71ms/step - loss: 0.4816 - accuracy: 0.8699 - val_loss: 2.2302 - val_accuracy: 0.6753\n",
            "Epoch 19/30\n",
            "1126/1126 [==============================] - 81s 72ms/step - loss: 0.4590 - accuracy: 0.8755 - val_loss: 2.2536 - val_accuracy: 0.6776\n",
            "Epoch 20/30\n",
            "1126/1126 [==============================] - 82s 72ms/step - loss: 0.4441 - accuracy: 0.8788 - val_loss: 2.2543 - val_accuracy: 0.6773\n",
            "Epoch 21/30\n",
            "1126/1126 [==============================] - 82s 72ms/step - loss: 0.4263 - accuracy: 0.8828 - val_loss: 2.3036 - val_accuracy: 0.6774\n",
            "Epoch 22/30\n",
            "1126/1126 [==============================] - 82s 72ms/step - loss: 0.4092 - accuracy: 0.8874 - val_loss: 2.3231 - val_accuracy: 0.6791\n",
            "Epoch 23/30\n",
            "1126/1126 [==============================] - 82s 73ms/step - loss: 0.3954 - accuracy: 0.8903 - val_loss: 2.3460 - val_accuracy: 0.6772\n",
            "Epoch 24/30\n",
            "1126/1126 [==============================] - 82s 73ms/step - loss: 0.3807 - accuracy: 0.8941 - val_loss: 2.4285 - val_accuracy: 0.6788\n",
            "Epoch 25/30\n",
            "1126/1126 [==============================] - 82s 72ms/step - loss: 0.3680 - accuracy: 0.8971 - val_loss: 2.3868 - val_accuracy: 0.6791\n",
            "Epoch 26/30\n",
            "1126/1126 [==============================] - 82s 73ms/step - loss: 0.3571 - accuracy: 0.9000 - val_loss: 2.4431 - val_accuracy: 0.6783\n",
            "Epoch 27/30\n",
            "1126/1126 [==============================] - 84s 74ms/step - loss: 0.3474 - accuracy: 0.9022 - val_loss: 2.5047 - val_accuracy: 0.6763\n",
            "Epoch 28/30\n",
            "1126/1126 [==============================] - 84s 75ms/step - loss: 0.3391 - accuracy: 0.9048 - val_loss: 2.4482 - val_accuracy: 0.6769\n",
            "Epoch 29/30\n",
            "1126/1126 [==============================] - 80s 71ms/step - loss: 0.3251 - accuracy: 0.9079 - val_loss: 2.5028 - val_accuracy: 0.6781\n",
            "Epoch 30/30\n",
            "1126/1126 [==============================] - 81s 72ms/step - loss: 0.3164 - accuracy: 0.9106 - val_loss: 2.5374 - val_accuracy: 0.6759\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Compile the model with Adam optimizer\n",
        "transformer.compile(\n",
        "    optimizer=Adam(),  # Using Adam optimizer\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model\n",
        "history = transformer.fit(train_ds, epochs=30, validation_data=val_ds)\n",
        "\n",
        "# Save the model\n",
        "transformer.save(\"transformer_model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtV-4Jp7cgNt"
      },
      "source": [
        "Translating new sentences with our Transformer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "lKCdvEQFcgWn"
      },
      "outputs": [],
      "source": [
        "sin_vocab = target_vectorization.get_vocabulary()\n",
        "sin_index_lookup = dict(zip(range(len(sin_vocab)), sin_vocab))\n",
        "max_decoded_sentence_length = 30\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P124OmIxcgeo"
      },
      "source": [
        "Output Testing and Decoding the output sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "xjO9rIZkcgnq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "     tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "     decoded_sentence = \"[start]\"\n",
        "     for i in range(max_decoded_sentence_length):\n",
        "       tokenized_target_sentence = target_vectorization(\n",
        "         [decoded_sentence])[:, :-1]\n",
        "       predictions = transformer(\n",
        "         [tokenized_input_sentence, tokenized_target_sentence])\n",
        "       sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "       sampled_token = sin_index_lookup[sampled_token_index]\n",
        "       decoded_sentence += \" \" + sampled_token\n",
        "       if sampled_token == \"[end]\": break\n",
        "     return decoded_sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzPCHf-Acgvm"
      },
      "source": [
        "Transformer translating output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "fmWCUkuscg4y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bb90d31-a1dc-47a5-a973-21eeb096c3e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-\n",
            "It is not a watch.\n",
            "[start] එය ඔරලෝසුවක් නොවේ [end]\n",
            "-\n",
            "Ask him his name.\n",
            "[start] ඔහුගේ නම අහන්න [end]\n",
            "-\n",
            "Is it private?\n",
            "[start] එය පෞද්ගලික [UNK] [end]\n",
            "-\n",
            "I smiled.\n",
            "[start] මම සිනාසුණේය [end]\n",
            "-\n",
            "Tom handed a note to Mary.\n",
            "[start] ටොම් මලක් ගෙනැවිත් ටෙම්ට [end]\n",
            "-\n",
            "The grapefruit tastes very sour.\n",
            "[start] ගෙඩි ඉතා ඇඹුල් රසයි [end]\n",
            "-\n",
            "Tom is going to break up with Mary.\n",
            "[start] ටොම් මරියා සමඟ පිටතට යාමට යන්නේ ය [end]\n",
            "-\n",
            "I can't remember.\n",
            "[start] මට මතකයි [end]\n",
            "-\n",
            "It's a classic.\n",
            "[start] එය [UNK] [end]\n",
            "-\n",
            "Tom has only done half his homework.\n",
            "[start] ටොම් සිය ගෙදර වැඩ කටයුතු සඳහා කර ඇත [end]\n"
          ]
        }
      ],
      "source": [
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(10):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    print(\"-\")\n",
        "    print(input_sentence)\n",
        "    print(decode_sequence(input_sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r660q0N3chCL"
      },
      "source": [
        "Evaluation using the BLEU score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Cs6FBdvQchK-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afd41bfe-191c-43f5-cd14-418990b6e742"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[start] ටොම් බාල්දිය වතුරෙන් පුරවා ගත්තේය [end] [start] ටොම් බාල්දිය ඉහළට පුරවා ඇත. [end]\n",
            "Score:0.5319148936170213\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[start] එලාම් සක්රිය කිරීම සඳහා මෙම මෘදු the ය යුතුය [end] [start] සංකේතාංකන තාක්ෂණය එය තරමක් විශ්වාසදායක තැන දක්වා ඉදිරියට ගොස් ඇත. [end]\n",
            "Score:0.3620689655172414\n",
            "[start] මෙය දුෂ්කර බව මම දනිමි [end] [start] මම දන්නවා මේක අමාරුයි කියලා. [end]\n",
            "Score:0.5555555555555556\n",
            "[start] අපි මෙම ආකාරයේ බළලෙකුට අක්ෂර වින්යාසයක් දමන්න [end] [start] මෙම ආකාරයේ සිදුවීමක් පුනරාවර්තනය වීම අප වළක්වා ගත යුතුය. [end]\n",
            "Score:0.4745762711864407\n",
            "[start] මම මගේ නිවාඩුව දෙස බලා ගත්තා [end] [start] මම මගේ නිවාඩුව හකාන් එකක ගත කළා. [end]\n",
            "Score:0.5\n",
            "[start] මම හිතුවේ ඔයාට ටොම්ට කරන්න ඕන කියලා [end] [start] මම හිතුවේ ඔයාට ටොම් සමඟ සාදයට යන්න ඕන කියලා. [end]\n",
            "Score:0.5510204081632653\n",
            "[start] අපි කෑම වලින් එළියට ගියා [end] [start] අපි කෑමෙන් ඉවරයි. [end]\n",
            "Score:0.5263157894736842\n",
            "[start] එය කාලය කුමක්ද කියා මම කල්පනා කරමි [end] [start] මම කල්පනා කරන්නේ එය කුමන වේලාවක්ද යන්නයි. [end]\n",
            "Score:0.4791666666666667\n",
            "[start] මතක තබා ගැනීම නවත්වන්න [end] [start] කලබල වීම නවත්වන්න. [end]\n",
            "Score:0.5\n",
            "[start] මේක ගොඩක් නරකයි [end] [start] මේක හරිම නරකයි. [end]\n",
            "Score:0.5862068965517241\n",
            "[start] අපේ දරුවන්ට යමක් කිරීමට යමක් අවශ්යයි [end] [start] අපේ දරුවන්ට යමක් කිරීමට අවශ්යයි. [end]\n",
            "Score:0.52\n",
            "[start] මෙම හෝටලය පිළිබඳ අපූරු දර්ශනයක් ඇත [end] [start] මෙම හෝටලයට මුහුද පිළිබඳ විශ්මය ජනක දෘෂ්ටියක් ඇත. [end]\n",
            "Score:0.625\n",
            "[start] ඔබට එය හැසිරවිය යුතුව තිබුණි [end] [start] ඔබ එය මැන තිබේද? [end]\n",
            "Score:0.40476190476190477\n",
            "[start] එයට ඒකට කිසිවක් නැත [end] [start] එයට එයට කිසිදු සම්බන්ධයක් නැත. [end]\n",
            "Score:0.6060606060606061\n",
            "[start] මම මගේ වයස ගැන ඔහුට බොරු කිව්වා [end] [start] මම මගේ වයස ගැන මගේ පෙම්වතාට බොරු කිව්වා. [end]\n",
            "Score:0.6\n",
            "[start] අපගේ සාර්ථකත්වයට ඊර්ෂ්යා කරයි [end] [start] ඒවා අපගේ සාර්ථකත්වයට ඊර්ෂ්යා කරයි. [end]\n",
            "Score:0.627906976744186\n",
            "[start] ටොම්ව දකින සෑම තැනකම බලා සිටින අතර ටොම්ගේ [UNK] සොයාගත නොහැකි විය [end] [start] පොලිසිය සෑම තැනකම බලා සිටි අතර ටොම්ගේ කිසිදු හෝඩුවාවක් සොයාගත නොහැකි විය. [end]\n",
            "Score:0.4050632911392405\n",
            "[start] යමක් සිදුවී ඇත [end] [start] යමක් සිදු විය යුතුය. [end]\n",
            "Score:0.7142857142857143\n",
            "[start] ටොම් යනු වගකිවයුතු රියදුරු ය [end] [start] ටොම් යනු නොසැලකිලිමත් රියදුරෙකි. [end]\n",
            "Score:0.5238095238095238\n",
            "[start] ටොම් මරියා මෙතන මොකද කළේ කියලා කල්පනා කළා [end] [start] ටොම් මරියා මෙතන කරන්නේ කුමක්දැයි කල්පනා කළේය. [end]\n",
            "Score:0.4909090909090909\n",
            "\n",
            "Scaled BLEU score : 52.92/100\n"
          ]
        }
      ],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "test_sin_texts = [pair[1] for pair in test_pairs]\n",
        "score = 0\n",
        "bleu  = 0\n",
        "for i in range(20):\n",
        "    candidate = decode_sequence(test_eng_texts[i])\n",
        "    reference = test_sin_texts[i].lower()\n",
        "    print(candidate,reference)\n",
        "    score = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))\n",
        "    bleu += score\n",
        "    print(f\"Score:{score}\")\n",
        "scaled_bleu = (bleu / 20) * 100\n",
        "print(f\"\\nScaled BLEU score : {round(scaled_bleu,2)}/100\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GkdxaKBBT6z"
      },
      "source": [
        "Calculate accuracy for 20 new sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "3KQm5sjOBUIR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d8dd6c0-d769-4936-c08c-f9cb46301fa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[start] මම මගේ ඇඳුම වියළි පිරිසිදු [UNK] යැවීමට කැමතියි [end] මම මගේ ඇඳුම වියළි පිරිසිදු කරන්නෙකුට යැවීමට කැමතියි.\n",
            "Score: 0.32786885245901637\n",
            "[start] ටොම් හාස්යජනක වේ [end] ටොම් හාස්යජනකයි.\n",
            "Score: 0.4\n",
            "[start] එය තරමක් කැපී පෙනේ [end] එය තරමක් කැපී පෙනේ.\n",
            "Score: 0.4375\n",
            "[start] ටොම් සතුටු නොවීය [end] ටොම් සතුටු නොවීය.\n",
            "Score: 0.4\n",
            "[start] මම මීට ඉක්මනින් කවදාවත් මේ වගේ නැහැ [end] මම මේ වේලාසනින් මේ කවදාවත් අවදි කළේ නැහැ.\n",
            "Score: 0.2653061224489796\n",
            "[start] එම අවස්ථාවන්හිදී රළුබව වළක්වා ගත යුතුය [end] එම අවස්ථාවන්හිදී රළුබව වළක්වා ගත යුතුය.\n",
            "Score: 0.4230769230769231\n",
            "[start] ඔහු තම දුක සිනහවක් පිටුපස සඟවා ගත්තේය [end] ඔහු තම දුක සිනහවක් පිටුපස සඟවා ගත්තේය.\n",
            "Score: 0.39215686274509803\n",
            "[start] ටොම් දැනට ප්රමාණවත් තරම් මුදල් උපයන්නේ නැත [end] ටොම් දැනට ඔහු පුරුදු වී ඇති තරම් මුදල් උපයන්නේ නැත.\n",
            "Score: 0.30357142857142855\n",
            "[start] මගේ උපකාරය මට මගේ සාර්ථකත්වයට ණයගැතියි [end] ඇගේ උදව්වට මගේ සාර්ථකත්වයට මම ණයගැතියි.\n",
            "Score: 0.3461538461538461\n",
            "[start] සමහර වෙළඳසැල් මඟින් මිල වට්ටම් කරයි [end] සමහර වෙළඳසැල් මඟින් මිල වට්ටම් කරයි.\n",
            "Score: 0.36734693877551017\n",
            "[start] ඇය [UNK] නව පරිගණකයක් ක්රියාත්මක සෙනසුරාදා කමිසයට තැබුවාය [end] ඇය කාමරයේ නව විදුලි උදුනක් සවි කළාය.\n",
            "Score: 0.19718309859154928\n",
            "[start] මම අසමත් වුණා [end] මම අසමත් වුණා.\n",
            "Score: 0.37037037037037035\n",
            "[start] අපි සිහින දකින්නේ එය [end] එය සත්ය දැයි විමසමු.\n",
            "Score: 0.20588235294117646\n",
            "[start] ඔබ කියන දේ මට තේරුම් ගත නොහැක [end] මට තේරෙනවා ඔයා කියන දේ.\n",
            "Score: 0.27906976744186046\n",
            "[start] මට [UNK] අවශ්යයි [end] මට ඉන්ද්රජාලිකයෙකු වීමට අවශ්යයි.\n",
            "Score: 0.3\n",
            "[start] අපි හෙට කාලගුණික අවසරය [UNK] [end] අපි හෙට පිටත් වෙමු, කාලගුණික අවසරය.\n",
            "Score: 0.40476190476190477\n",
            "[start] වසන්තයේ ලබා ගැනීම ඔබට බඩගිනි නම් ඔබේ කාර්යාලය දැන ගන්නවාද [end] වසන්ත රෝල් වසන්ත රෝල් ලෙස හැඳින්වේ ඇයි කියා ඔබ දන්නවාද?\n",
            "Score: 0.23943661971830987\n",
            "[start] ඔබ මා සමඟ මුළුමනින්ම අවංක වී සිටියාද [end] ඔබ මා සමඟ මුළුමනින්ම අවංක වී සිටියාද?\n",
            "Score: 0.4\n",
            "[start] මුදල් ලෝකය පාලනය කරයි [end] මුදල් ලෝකය පාලනය කරයි.\n",
            "Score: 0.4\n",
            "[start] විදේශ භාෂා ප්රගුණ කිරීමට ඇති එකම ක්රමය [UNK] [end] විදේශ භාෂා ප්රගුණ කිරීමට ඇති එකම ක්රමය පුරුද්දයි.\n",
            "Score: 0.39655172413793105\n",
            "\n",
            "Scaled BLEU score : 34.28/100\n"
          ]
        }
      ],
      "source": [
        "manualTest = [\n",
        "    (\"I'd like to send my dress to a dry cleaner.\", \"මම මගේ ඇඳුම වියළි පිරිසිදු කරන්නෙකුට යැවීමට කැමතියි.\"),\n",
        "    (\"Tom is ridiculous.\", \"ටොම් හාස්යජනකයි.\"),\n",
        "    (\"That's quite remarkable.\", \"එය තරමක් කැපී පෙනේ.\"),\n",
        "    (\"Tom wasn't happy.\", \"ටොම් සතුටු නොවීය.\"),\n",
        "    (\"I've never woken up this early.\", \"මම මේ වේලාසනින් මේ කවදාවත් අවදි කළේ නැහැ.\"),\n",
        "    (\"Harshness should be avoided in those cases.\", \"එම අවස්ථාවන්හිදී රළුබව වළක්වා ගත යුතුය.\"),\n",
        "    (\"He hid his sadness behind a smile.\", \"ඔහු තම දුක සිනහවක් පිටුපස සඟවා ගත්තේය.\"),\n",
        "    (\"Tom currently doesn't make as much money as he used to.\", \"ටොම් දැනට ඔහු පුරුදු වී ඇති තරම් මුදල් උපයන්නේ නැත.\"),\n",
        "    (\"I owe my success to her help.\", \"ඇගේ උදව්වට මගේ සාර්ථකත්වයට මම ණයගැතියි.\"),\n",
        "    (\"Some stores discount the price.\", \"සමහර වෙළඳසැල් මඟින් මිල වට්ටම් කරයි.\"),\n",
        "    (\"She installed a new electric stove in the room.\", \"ඇය කාමරයේ නව විදුලි උදුනක් සවි කළාය.\"),\n",
        "    (\"I have failed.\", \"මම අසමත් වුණා.\"),\n",
        "    (\"Let's ask if it's true.\", \"එය සත්ය දැයි විමසමු.\"),\n",
        "    (\"I can't understand what you're saying.\", \"මට තේරෙනවා ඔයා කියන දේ.\"),\n",
        "    (\"I want to be a magician.\", \"මට ඉන්ද්රජාලිකයෙකු වීමට අවශ්යයි.\"),\n",
        "    (\"We shall leave tomorrow, weather permitting.\", \"අපි හෙට පිටත් වෙමු, කාලගුණික අවසරය.\"),\n",
        "    (\"Do you know why spring rolls are called spring rolls?\", \"වසන්ත රෝල් වසන්ත රෝල් ලෙස හැඳින්වේ ඇයි කියා ඔබ දන්නවාද?\"),\n",
        "    (\"Have you been totally honest with me?\", \"ඔබ මා සමඟ මුළුමනින්ම අවංක වී සිටියාද?\"),\n",
        "    (\"Money rules the world.\", \"මුදල් ලෝකය පාලනය කරයි.\"),\n",
        "    (\"Practice is the only way to master foreign languages.\", \"විදේශ භාෂා ප්රගුණ කිරීමට ඇති එකම ක්රමය පුරුද්දයි.\")\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "testENTexts = [pair[0] for pair in manualTest]\n",
        "testSITexts = [pair[1] for pair in manualTest]\n",
        "scores = []\n",
        "bleu  = 0\n",
        "for i in range(20):\n",
        "    candidate = decode_sequence(testENTexts[i])\n",
        "    reference = testSITexts[i].lower()\n",
        "    print(candidate, reference)\n",
        "    score = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))\n",
        "    scores.append(score)\n",
        "    bleu += score\n",
        "    print(f\"Score: {score}\")\n",
        "scaled_bleu = (bleu / 20) * 100\n",
        "print(f\"\\nScaled BLEU score : {round(scaled_bleu, 2)}/100\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}